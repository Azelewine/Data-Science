{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YI_eQRc0G2kg",
        "uqhyQC8ZHRnK",
        "qDwO7b0bJnSL",
        "sXy-GRf8ZB-Y",
        "w3PuYHdMM7oU",
        "K8WWjGM4dhy2",
        "L4kW8RTrgcl3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Business problem**"
      ],
      "metadata": {
        "id": "K0dklzh5BgUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My company is a shipping company in charge of shipping different goods from different companies and different customers. Alot of email messaging is used internally and externally with customers. In a day the tracking department deals with more than 1000 emails. Some of these emails are not important emails which takes time to read. Also as time is wasted on unimportant emails it delays responding time to customers of which they are complaining about. As the data scientist in the company, I will design a system to make is easier to categorize and seperate important from unimportant emails.\n",
        "\n",
        "**Benefits to the company**\n",
        "\n",
        "*   Response to customers will be faster which will improve the image of the company.\n",
        "*   Increase productivity\n",
        "*   Categorizing into spam can block fraud attempts.\n",
        "*   Improve customer satisfaction.\n",
        "\n",
        "\n",
        "**Formulate as NLP**\n",
        "\n",
        "Statistics, python, etc. skills will be used. It will be approached as text classification task which will be preprocessed, trained using a traditional method and deep learning method before testing and evaluting. This task can be used in messaging platforms.\n",
        "\n",
        "\n",
        "Data is taken from kaggle."
      ],
      "metadata": {
        "id": "rvsND-24Bfh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Components of NLP system**"
      ],
      "metadata": {
        "id": "7NjOlvY0qPrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main components of this task includes the following\n",
        "\n",
        "*   Text preprocessing which involves tokenization, coverting letters to lower case, stopword removal and lemmatization. This step is essential as it cleans the raw data for analysis.\n",
        "*   Text representation which transforms the cleaned data into numerical features for traditional models and deep learning models.\n",
        "* Machine learning(logistic regression) and deep learning models(LSTM) will be used predict and perform task. Since this is to detect spam and not spam these models will be used.  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G86dHr-tqXci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading libraries**"
      ],
      "metadata": {
        "id": "YI_eQRc0G2kg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh2l-_5DC9Bz",
        "outputId": "788277e8-72ee-4ff2-851d-b3f1b5d4c6de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "import pandas as pd\n",
        "!pip install nltk\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.layers import Input,Embedding, LSTM, Bidirectional, Dropout, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The libraries are loaded"
      ],
      "metadata": {
        "id": "2LX3B-6dc-lU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load dataset**"
      ],
      "metadata": {
        "id": "uqhyQC8ZHRnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/Spam Email raw text for NLP.csv\")"
      ],
      "metadata": {
        "id": "rKqkhnDtDeZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XkUDdOOD3XM",
        "outputId": "690c41b4-501a-4335-b360-745b3540a894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['CATEGORY', 'MESSAGE', 'FILE_NAME'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fq9ifpiE6Tt",
        "outputId": "31a3a1f9-90e7-422d-9ec8-30990e0b9560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5796, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is loaded and it contains 3 columns."
      ],
      "metadata": {
        "id": "Xjy_JJkwdI1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text preprocessing**"
      ],
      "metadata": {
        "id": "qDwO7b0bJnSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(data):\n",
        "    try:\n",
        "        data = re.sub(r'[^a-z\\s]', '', data.lower())\n",
        "        tokens = word_tokenize(data)\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "    except AttributeError:\n",
        "        print(f\"Skipping non-string value: {data}\")\n",
        "        return \"\"\n",
        "\n",
        "data['cleaned_data_text'] = data['MESSAGE'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "ZvqW3NxjMwqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This coverts all uppercase letters to lowercase and removes non-alphabetic characters. It is then split into tokens. Followed by the lemmatization step which take words to the their root form.\n",
        "\n",
        "This is process is important because it cleans the data, removes irrelevant words in a standard form for a better model training"
      ],
      "metadata": {
        "id": "E_RBKB4IJz91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[['MESSAGE', 'cleaned_data_text']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-mdb0K7JUM-",
        "outputId": "80d221d8-2cad-4204-87c2-5b9cc3ca6322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             MESSAGE  \\\n",
            "0  Dear Homeowner,\\n\\n \\n\\nInterest Rates are at ...   \n",
            "1  ATTENTION: This is a MUST for ALL Computer Use...   \n",
            "2  This is a multi-part message in MIME format.\\n...   \n",
            "3  IMPORTANT INFORMATION:\\n\\n\\n\\nThe new domain n...   \n",
            "4  This is the bottom line.  If you can GIVE AWAY...   \n",
            "\n",
            "                                   cleaned_data_text  \n",
            "0  dear homeowner interest rate lowest point year...  \n",
            "1  attention must computer user newspecial packag...  \n",
            "2  multipart message mime format nextpartcdccbfa ...  \n",
            "3  important information new domain name finally ...  \n",
            "4  bottom line give away cd free people like one ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data['cleaned_data_text']\n",
        "y = data['CATEGORY']"
      ],
      "metadata": {
        "id": "Br3h-NzgLA_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into features and labels where X represents features and y the labels."
      ],
      "metadata": {
        "id": "50QvZMj3RkJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Train size: {len(X_train)}\")\n",
        "print(f\"Test  size: {len(X_test)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WrtrvVmNOZO",
        "outputId": "826e1d47-48a6-4beb-d0c6-35ee93596a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 4636\n",
            "Test  size: 1160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is split into train(80%) and test(20%)."
      ],
      "metadata": {
        "id": "InzepmbMTZIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(max_features=5000)\n",
        "\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "liOZ-DitPSHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This coverts the data into a numeric bag-of-words(which captures how frequently a word appears). This will be used on the traditional method."
      ],
      "metadata": {
        "id": "34Z4PO4JX3Fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Traditional method**"
      ],
      "metadata": {
        "id": "sXy-GRf8ZB-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "para_gri = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "log_re = LogisticRegression()\n",
        "grid_srh = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_srh.fit(X_train_vectorized, y_train)\n",
        "\n",
        "model = grid_search.best_estimator_\n",
        "print(\"Best Parameters:\", grid_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOHcNJTfcpPo",
        "outputId": "c160f589-8dff-4a68-ff44-add3ebc02bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the traditional training method logistics method is used. A grid search with cross evaluation is performed. This is done to get the best parameters to build the best model for better results."
      ],
      "metadata": {
        "id": "Xm2ucXdUdo6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_model.predict(X_test_vectorized)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6TQixsagF1W",
        "outputId": "6fbca268-0654-4f29-eb6a-f378f6e212ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model with the best parameters are then evaluated on the test set to get the evaluation metrics which is the accuracy."
      ],
      "metadata": {
        "id": "QWRw8erjfzQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTDnMgHGjfP7",
        "outputId": "3f0291d6-cb9a-4f49-ee7a-094672855933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[756   6]\n",
            " [  3 395]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion metrics is performed to evaluate the performance. This results shows that the model correctly predicted  756 emails were spam and 395 emails were not spam. The value 6 shows that 6 emails where predicted as not spam but are actually spam.The value 3 predicted emails as spam but its not spam.\n",
        "\n",
        "Since there is less false positive and false negtive values, this shows that model can categorize the classes."
      ],
      "metadata": {
        "id": "AWH1-ffYkjgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep learning method**"
      ],
      "metadata": {
        "id": "w3PuYHdMM7oU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(data['cleaned_data_text'])"
      ],
      "metadata": {
        "id": "yw-xhV19eDtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This converts the words in the data into tokens for deep learning models."
      ],
      "metadata": {
        "id": "Q6J8pSXcqO7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "8M2p3JB0g8ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training and test data is transformed into a sequence of numerical value which is important for training the LSTM model."
      ],
      "metadata": {
        "id": "MjLCZ1U_yt17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max(len(seq) for seq in X_train_sequences)\n",
        "X_train_sequences = pad_sequences(X_train_sequences, maxlen=max_length, padding='post')\n",
        "X_test_sequences = pad_sequences(X_test_sequences, maxlen=max_length, padding='post')"
      ],
      "metadata": {
        "id": "Xat49oVwkuLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The x train and test sequences are padded. This means that all sequences will have the same length and shape."
      ],
      "metadata": {
        "id": "tqQWT1pC0EqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n"
      ],
      "metadata": {
        "id": "DdQsCILD0AQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The labels are converted to numpy arrays."
      ],
      "metadata": {
        "id": "zbz1qvv81Ewk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Input(shape=(None,)),\n",
        "    Embedding(input_dim=5000, output_dim=16),\n",
        "    Bidirectional(LSTM(16, return_sequences=False)),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "Uf6v_Ec_V6HL",
        "outputId": "c871bf44-6278-4129-af1a-ffc69811bdec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_12 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)            │          \u001b[38;5;34m80,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_12 (\u001b[38;5;33mBidirectional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m4,224\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">80,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m84,257\u001b[0m (329.13 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">84,257</span> (329.13 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m84,257\u001b[0m (329.13 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">84,257</span> (329.13 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An LSTM model is used to train the data. This model has an embedded layer which considers a words of size 5000, with each word being represented as a vector of size 32. The bidirectional layer which contain 32 dimensions and output only the last hidden state. Dropout is perforemed to prevent overfitting with a dense layer of 1 dimenstion since the task is to classify into spam or not spam.\n",
        "The model is then compiled using the loss function binary crossentropy which is used for binary classification tasks."
      ],
      "metadata": {
        "id": "mZflg4qa3CZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_sequences, y_train, epochs=5, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG3Q3P1OWLPw",
        "outputId": "883cd380-c9fb-4e42-9acf-dc679242cb67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1591s\u001b[0m 13s/step - accuracy: 0.7204 - loss: 0.5801 - val_accuracy: 0.9688 - val_loss: 0.1962\n",
            "Epoch 2/5\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1530s\u001b[0m 13s/step - accuracy: 0.9858 - loss: 0.1535 - val_accuracy: 0.9806 - val_loss: 0.0876\n",
            "Epoch 3/5\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1532s\u001b[0m 13s/step - accuracy: 0.9921 - loss: 0.0761 - val_accuracy: 0.9720 - val_loss: 0.0887\n",
            "Epoch 4/5\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1573s\u001b[0m 13s/step - accuracy: 0.9970 - loss: 0.0465 - val_accuracy: 0.9828 - val_loss: 0.0724\n",
            "Epoch 5/5\n",
            "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1550s\u001b[0m 13s/step - accuracy: 0.9970 - loss: 0.0329 - val_accuracy: 0.9828 - val_loss: 0.0709\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7dabf04c0d60>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy for this model is 0.9970."
      ],
      "metadata": {
        "id": "BI6Gy07lc4iQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation**"
      ],
      "metadata": {
        "id": "K8WWjGM4dhy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test_sequences, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnoSm0_AdEGJ",
        "outputId": "7cd855b1-cdf3-41cc-e609-2d44edcaf7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 742ms/step - accuracy: 0.9826 - loss: 0.0690\n",
            "Test Loss: 0.0530\n",
            "Test Accuracy: 0.9862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is evaluated using the test data."
      ],
      "metadata": {
        "id": "i9jk1VUqdmqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model prediction**"
      ],
      "metadata": {
        "id": "L4kW8RTrgcl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\"I cannot track my package.\"]\n",
        "message_seq = tokenizer.texts_to_sequences(message)\n",
        "message_padded = pad_sequences(message_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "prediction = model.predict(new_text_padded)\n",
        "print(f\"Prediction: {'Spam' if prediction > 0.5 else 'Not Spam'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQwr6hoXeS5P",
        "outputId": "c264416f-7091-454f-cc1c-86eb7ff2e70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step\n",
            "Prediction: Not Spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the trained model, the message above is predicted as not a spam as the predicted value is less than 0.5."
      ],
      "metadata": {
        "id": "MGDFKTiHg39O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "Q_QQ92v7_LWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overall Pipeline**\n",
        "\n",
        "This is a text classification task. This involves data loading, text preprocessing, model building and training using a traditional method(Logistics regression) and a deep learning method(LSTM), testing, evaluating and predicting. This  is possible using sklearn, nltk and TensorFlow.\n",
        "\n",
        "**Limitations**\n",
        "\n",
        "*   Training the LSTM model is time consumming.\n",
        "\n",
        "**Strengths**\n",
        "\n",
        "*   Logistics regression is simple and fast.\n",
        "*   LSTM performs better.\n",
        "\n",
        "**Implication**\n",
        "\n",
        "*  This shows that emails can be separated to spam and not spam which will reduce the time wasted to read every mail.\n",
        "*   Less complains from customers on delayed response.\n",
        "\n",
        "**Recommendation**\n",
        "\n",
        "Gather information old emails and group them as important and not important. Preprocess and train a model.\n",
        "\n",
        "**Reference**\n",
        "\n",
        "https://www.kaggle.com/datasets/chandramoulinaidu/spam-classification-for-basic-nlp/data"
      ],
      "metadata": {
        "id": "33wuX7l4_P_u"
      }
    }
  ]
}